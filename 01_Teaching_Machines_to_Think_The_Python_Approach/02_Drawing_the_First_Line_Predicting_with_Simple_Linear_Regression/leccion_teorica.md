# ğŸ“Š RegresiÃ³n Lineal Simple - TeorÃ­a Completa

**MÃ³dulo 1: Machine Learning with Python - LecciÃ³n 02**

---

## ğŸ“‘ Tabla de Contenidos

1. [IntroducciÃ³n](#1-introducciÃ³n)
2. [Â¿QuÃ© es la RegresiÃ³n Lineal?](#2-quÃ©-es-la-regresiÃ³n-lineal)
3. [La MatemÃ¡tica DetrÃ¡s: EcuaciÃ³n de la Recta](#3-la-matemÃ¡tica-detrÃ¡s-ecuaciÃ³n-de-la-recta)
4. [MÃ©todo de MÃ­nimos Cuadrados Ordinarios (OLS)](#4-mÃ©todo-de-mÃ­nimos-cuadrados-ordinarios-ols)
5. [MÃ©tricas de EvaluaciÃ³n](#5-mÃ©tricas-de-evaluaciÃ³n)
6. [Supuestos de la RegresiÃ³n Lineal](#6-supuestos-de-la-regresiÃ³n-lineal)
7. [InterpretaciÃ³n de Resultados](#7-interpretaciÃ³n-de-resultados)
8. [AnÃ¡lisis de Residuos](#8-anÃ¡lisis-de-residuos)
9. [Limitaciones y CuÃ¡ndo NO Usar RegresiÃ³n Lineal](#9-limitaciones-y-cuÃ¡ndo-no-usar-regresiÃ³n-lineal)
10. [Aplicaciones en el Mundo Real](#10-aplicaciones-en-el-mundo-real)

---

## 1. IntroducciÃ³n

La **regresiÃ³n lineal simple** es uno de los algoritmos mÃ¡s fundamentales y ampliamente utilizados en Machine Learning y estadÃ­stica. Es el primer paso para entender modelos mÃ¡s complejos y proporciona una base sÃ³lida para el aprendizaje supervisado.

### Â¿Por quÃ© es importante?

- ğŸ¯ **Fundamento**: Base para algoritmos mÃ¡s avanzados
- ğŸ“ˆ **Interpretable**: FÃ¡cil de entender y explicar
- âš¡ **RÃ¡pido**: Computacionalmente eficiente
- ğŸ”§ **VersÃ¡til**: Aplicable a muchos problemas
- ğŸ“Š **Visual**: Resultados fÃ¡ciles de visualizar

### Contexto HistÃ³rico

La regresiÃ³n lineal fue desarrollada por **Francis Galton** en el siglo XIX mientras estudiaba la herencia de caracterÃ­sticas fÃ­sicas. El tÃ©rmino "regresiÃ³n" viene de su observaciÃ³n de que las alturas de los hijos tendÃ­an a "regresar" hacia la media poblacional.

---

## 2. Â¿QuÃ© es la RegresiÃ³n Lineal?

### DefiniciÃ³n Formal

La **regresiÃ³n lineal simple** es un mÃ©todo estadÃ­stico que modela la relaciÃ³n lineal entre:

- Una **variable independiente** (X) - tambiÃ©n llamada predictor o feature
- Una **variable dependiente** (y) - tambiÃ©n llamada respuesta o target

### AnalogÃ­a del Mundo Real

Imagina que quieres predecir tu calificaciÃ³n final basÃ¡ndote en las horas de estudio:

```
Horas de Estudio (X) â†’ [MODELO] â†’ CalificaciÃ³n (y)
```

Si estudias mÃ¡s horas, esperas obtener una mejor calificaciÃ³n. La regresiÃ³n lineal encuentra la **mejor lÃ­nea recta** que representa esta relaciÃ³n.

### Objetivo

Encontrar la ecuaciÃ³n de una lÃ­nea recta que:

1. **Mejor ajuste** los datos observados
2. **Minimice** el error entre predicciones y valores reales
3. **Permita** hacer predicciones para nuevos valores

### Tipos de Relaciones

```
Positiva: X â†‘ â†’ y â†‘     Negativa: X â†‘ â†’ y â†“     Sin relaciÃ³n: X â†‘ â†’ y = ?
    y                        y                         y
    |    /                   |  \                      |  â€¢ â€¢  â€¢
    |   /                    |   \                     | â€¢  â€¢ â€¢
    |  /                     |    \                    |â€¢ â€¢  â€¢
    | /                      |     \                   |  â€¢ â€¢ â€¢
    |/_____x                 |______\x                 |_______x
```

---

## 3. La MatemÃ¡tica DetrÃ¡s: EcuaciÃ³n de la Recta

### EcuaciÃ³n Fundamental

```
y = mx + b
```

Donde:

- **y**: Variable dependiente (lo que predecimos)
- **x**: Variable independiente (lo que conocemos)
- **m**: Pendiente (coeficiente) - cuÃ¡nto cambia y por cada unidad de x
- **b**: Intercepto - valor de y cuando x = 0

### En NotaciÃ³n de Machine Learning

```
Å· = Î²â‚€ + Î²â‚x
```

Donde:

- **Å·** (y-hat): Valor predicho
- **Î²â‚€** (beta cero): Intercepto
- **Î²â‚** (beta uno): Coeficiente

### Ejemplo NumÃ©rico

Supongamos que descubrimos:

```
Salario = 30,000 + 5,000 Ã— AÃ±os_Experiencia
```

**InterpretaciÃ³n:**

- Si tienes **0 aÃ±os** de experiencia: Salario = $30,000 (intercepto)
- Por cada **aÃ±o adicional**: Salario aumenta $5,000 (pendiente)
- Con **5 aÃ±os**: Salario = 30,000 + 5,000Ã—5 = $55,000

### VisualizaciÃ³n

```
Salario ($)
    |
80k |                    â€¢
    |                 â€¢
60k |              â€¢
    |           â€¢
40k |        â€¢
    |     â€¢
20k |  â€¢
    |___________________
      0  2  4  6  8  10
         AÃ±os Experiencia
```

---

## 4. MÃ©todo de MÃ­nimos Cuadrados Ordinarios (OLS)

### Â¿QuÃ© es OLS?

**Ordinary Least Squares** es el mÃ©todo mÃ¡s comÃºn para encontrar los mejores valores de **m** y **b**.

### Objetivo: Minimizar el Error

Queremos minimizar la **suma de residuos al cuadrado**:

```
SSE = Î£(yi - Å·i)Â²
```

Donde:

- **yi**: Valor real
- **Å·i**: Valor predicho
- **(yi - Å·i)**: Residuo (error)

### Â¿Por quÃ© al Cuadrado?

1. **Elimina signos negativos**: -5 y +5 se tratan igual
2. **Penaliza errores grandes**: Un error de 10 pesa mÃ¡s que dos de 5
3. **MatemÃ¡ticamente conveniente**: FunciÃ³n derivable y convexa

### FÃ³rmulas para Calcular m y b

#### Pendiente (m):

```
       Î£[(xi - xÌ„)(yi - È³)]
m = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Î£[(xi - xÌ„)Â²]
```

TambiÃ©n se puede expresar como:

```
       Cov(x, y)
m = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Var(x)
```

#### Intercepto (b):

```
b = È³ - m Ã— xÌ„
```

Donde:

- **xÌ„**: Media de X
- **È³**: Media de y

### Ejemplo de CÃ¡lculo Manual

Datos:

```
X (horas): [1, 2, 3, 4, 5]
y (calif): [2, 4, 5, 4, 5]
```

**Paso 1: Calcular medias**

```
xÌ„ = (1+2+3+4+5)/5 = 3
È³ = (2+4+5+4+5)/5 = 4
```

**Paso 2: Calcular desviaciones y productos**

```
xi  yi  (xi-xÌ„)  (yi-È³)  (xi-xÌ„)(yi-È³)  (xi-xÌ„)Â²
1   2    -2      -2         4           4
2   4    -1       0         0           1
3   5     0       1         0           0
4   4     1       0         0           1
5   5     2       1         2           4
                          â”€â”€          â”€â”€
                          6           10
```

**Paso 3: Calcular pendiente**

```
m = 6/10 = 0.6
```

**Paso 4: Calcular intercepto**

```
b = 4 - 0.6Ã—3 = 4 - 1.8 = 2.2
```

**EcuaciÃ³n resultante:**

```
y = 0.6x + 2.2
```

---

## 5. MÃ©tricas de EvaluaciÃ³n

### 1. RÂ² (Coeficiente de DeterminaciÃ³n)

**DefiniciÃ³n**: ProporciÃ³n de la varianza en y que es explicada por X.

```
       SS_res
RÂ² = 1 - â”€â”€â”€â”€â”€â”€
       SS_tot
```

Donde:

- **SS_res** = Î£(yi - Å·i)Â² (suma de residuos al cuadrado)
- **SS_tot** = Î£(yi - È³)Â² (varianza total)

**InterpretaciÃ³n:**

- **RÂ² = 1.0**: Modelo perfecto (explica 100% de la varianza)
- **RÂ² = 0.8**: Modelo muy bueno (explica 80%)
- **RÂ² = 0.5**: Modelo moderado (explica 50%)
- **RÂ² = 0.0**: Modelo no explica nada (igual a predecir la media)
- **RÂ² < 0.0**: Modelo peor que predecir la media

**Reglas generales:**

```
RÂ² > 0.9  â†’ Excelente
RÂ² > 0.7  â†’ Bueno
RÂ² > 0.5  â†’ Moderado
RÂ² < 0.3  â†’ Pobre
```

### 2. MSE (Error CuadrÃ¡tico Medio)

```
       Î£(yi - Å·i)Â²
MSE = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           n
```

- **Ventaja**: Penaliza errores grandes
- **Desventaja**: Unidades al cuadrado (difÃ­cil de interpretar)

### 3. RMSE (RaÃ­z del Error CuadrÃ¡tico Medio)

```
RMSE = âˆšMSE
```

- **Ventaja**: Mismas unidades que y (fÃ¡cil de interpretar)
- **Uso**: "El modelo se equivoca en promedio Â±X unidades"

### 4. MAE (Error Absoluto Medio)

```
       Î£|yi - Å·i|
MAE = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           n
```

- **Ventaja**: Menos sensible a outliers que MSE
- **InterpretaciÃ³n**: Error promedio sin considerar direcciÃ³n

### ComparaciÃ³n de MÃ©tricas

| MÃ©trica  | Unidades      | Sensibilidad Outliers | InterpretaciÃ³n           |
| -------- | ------------- | --------------------- | ------------------------ |
| **RÂ²**   | Ninguna (0-1) | Media                 | % varianza explicada     |
| **MSE**  | yÂ²            | Alta                  | Penaliza errores grandes |
| **RMSE** | y             | Alta                  | Error promedio           |
| **MAE**  | y             | Baja                  | Error absoluto promedio  |

### Ejemplo PrÃ¡ctico

PredicciÃ³n de precios de casas:

```python
y_real = [100k, 150k, 200k, 250k, 300k]
y_pred = [110k, 145k, 205k, 240k, 310k]

# Errores individuales
errores = [10k, -5k, 5k, -10k, 10k]

# MAE = (10 + 5 + 5 + 10 + 10) / 5 = 8k
# El modelo se equivoca en promedio Â±$8,000

# RMSE â‰ˆ 8.4k (ligeramente mayor por errores al cuadrado)
```

---

## 6. Supuestos de la RegresiÃ³n Lineal

Para que la regresiÃ³n lineal sea vÃ¡lida, los datos deben cumplir ciertos supuestos:

### 1. **Linealidad**

La relaciÃ³n entre X e y debe ser lineal.

```
âœ… Lineal          âŒ No lineal
y                  y
|  â€¢  â€¢            |     â€¢ â€¢
| â€¢  â€¢             |   â€¢     â€¢
|â€¢  â€¢              | â€¢         â€¢
|____x             |_____________x
```

**VerificaciÃ³n**: GrÃ¡fico de dispersiÃ³n

### 2. **Independencia**

Las observaciones deben ser independientes entre sÃ­.

**ViolaciÃ³n**: Datos de series temporales sin considerar autocorrelaciÃ³n

### 3. **Homoscedasticidad**

La varianza de los residuos debe ser constante.

```
âœ… HomoscedÃ¡stico    âŒ HeteroscedÃ¡stico
Residuos             Residuos
|  â€¢ â€¢ â€¢             |       â€¢ â€¢
| â€¢ â€¢ â€¢ â€¢            |     â€¢ â€¢ â€¢
|  â€¢ â€¢ â€¢             |   â€¢ â€¢ â€¢
|_______x            | â€¢ â€¢
                     |_______x
```

**VerificaciÃ³n**: GrÃ¡fico de residuos vs predicciones

### 4. **Normalidad de Residuos**

Los residuos deben distribuirse normalmente (especialmente importante para muestras pequeÃ±as).

**VerificaciÃ³n**: Histograma o Q-Q plot de residuos

### 5. **No Multicolinealidad** (para regresiÃ³n mÃºltiple)

Las variables independientes no deben estar altamente correlacionadas entre sÃ­.

### Consecuencias de Violar Supuestos

| Supuesto Violado  | Consecuencia                     |
| ----------------- | -------------------------------- |
| Linealidad        | Predicciones incorrectas         |
| Independencia     | Intervalos de confianza errÃ³neos |
| Homoscedasticidad | Errores estÃ¡ndar incorrectos     |
| Normalidad        | Pruebas de hipÃ³tesis invÃ¡lidas   |

---

## 7. InterpretaciÃ³n de Resultados

### Interpretar el Coeficiente (m)

**Ejemplo**: Salario = 30,000 + 5,000 Ã— AÃ±os_Experiencia

```
m = 5,000
```

**InterpretaciÃ³n correcta:**

> "Por cada aÃ±o adicional de experiencia, el salario aumenta en promedio $5,000, **manteniendo todo lo demÃ¡s constante**."

**Signos del coeficiente:**

- **m > 0**: RelaciÃ³n positiva (X â†‘ â†’ y â†‘)
- **m < 0**: RelaciÃ³n negativa (X â†‘ â†’ y â†“)
- **m = 0**: No hay relaciÃ³n lineal

### Interpretar el Intercepto (b)

```
b = 30,000
```

**InterpretaciÃ³n:**

> "El salario esperado para alguien con 0 aÃ±os de experiencia es $30,000."

**âš ï¸ Cuidado**: El intercepto solo tiene sentido si X=0 es un valor posible y realista.

### Interpretar RÂ²

**Ejemplo**: RÂ² = 0.75

**InterpretaciÃ³n correcta:**

> "El 75% de la variabilidad en los salarios puede explicarse por los aÃ±os de experiencia. El 25% restante se debe a otros factores no incluidos en el modelo."

**NO significa:**

- âŒ "El modelo es 75% preciso"
- âŒ "El modelo acierta el 75% de las veces"

---

## 8. AnÃ¡lisis de Residuos

### Â¿QuÃ© son los Residuos?

```
Residuo = Valor Real - Valor Predicho
    e   =    yi     -      Å·i
```

Los residuos nos dicen **quÃ© tan equivocado estÃ¡ el modelo** para cada observaciÃ³n.

### GrÃ¡ficos de Residuos

#### 1. Residuos vs Predicciones

```
âœ… Buen modelo          âŒ Mal modelo (patrÃ³n curvo)
Residuos                Residuos
|  â€¢ â€¢ â€¢                |      â€¢ â€¢
| â€¢ â€¢ â€¢ â€¢               |    â€¢     â€¢
|  â€¢ â€¢ â€¢                |  â€¢         â€¢
|_______Å·               |_____________Å·
```

**QuÃ© buscar:**

- âœ… Puntos distribuidos aleatoriamente alrededor de 0
- âŒ Patrones (curvas, embudos) indican problemas

#### 2. Q-Q Plot (Normalidad)

Compara la distribuciÃ³n de residuos con una distribuciÃ³n normal.

```
âœ… Normal              âŒ No normal
|     â€¢                |        â€¢
|   â€¢                  |      â€¢
| â€¢                    |   â€¢
|â€¢___                  | â€¢
                       |â€¢
```

### Diagnosticar Problemas

| PatrÃ³n en Residuos  | Problema            | SoluciÃ³n                               |
| ------------------- | ------------------- | -------------------------------------- |
| Curva en U          | No linealidad       | Transformar variables, usar polinomios |
| Embudo              | Heteroscedasticidad | Transformar y (log, sqrt)              |
| Puntos muy alejados | Outliers            | Investigar y posiblemente remover      |
| PatrÃ³n temporal     | AutocorrelaciÃ³n     | Modelos de series temporales           |

---

## 9. Limitaciones y CuÃ¡ndo NO Usar RegresiÃ³n Lineal

### Limitaciones Principales

1. **Solo relaciones lineales**
   - No captura curvas, exponenciales, logarÃ­tmicas
2. **Sensible a outliers**
   - Un punto extremo puede cambiar mucho la lÃ­nea
3. **ExtrappolaciÃ³n riesgosa**
   - Predecir fuera del rango de datos puede ser muy inexacto
4. **Asume relaciÃ³n constante**
   - No modela interacciones o cambios en el tiempo
5. **Necesita suficientes datos**
   - Con pocos puntos, el modelo no es confiable

### CuÃ¡ndo NO Usar RegresiÃ³n Lineal

âŒ **RelaciÃ³n claramente no lineal**

```python
# Ejemplo: Crecimiento exponencial
# y = e^x no puede modelarse bien con y = mx + b
```

âŒ **Datos categÃ³ricos como target**

```python
# Mal: y = "Aprobado" / "Reprobado"
# Usar: RegresiÃ³n LogÃ­stica
```

âŒ **Muchos outliers extremos**

```python
# Considerar: RegresiÃ³n robusta o remover outliers
```

âŒ **Datos con estructura temporal compleja**

```python
# Considerar: Modelos ARIMA, Prophet
```

âŒ **ViolaciÃ³n severa de supuestos**

```python
# Verificar residuos antes de confiar en el modelo
```

### Alternativas

| SituaciÃ³n                   | Alternativa                   |
| --------------------------- | ----------------------------- |
| RelaciÃ³n curva              | RegresiÃ³n polinomial, Splines |
| MÃºltiples variables         | RegresiÃ³n lineal mÃºltiple     |
| ClasificaciÃ³n               | RegresiÃ³n logÃ­stica, Ã¡rboles  |
| Series temporales           | ARIMA, Prophet                |
| Datos no lineales complejos | Random Forest, XGBoost        |

---

## 10. Aplicaciones en el Mundo Real

### 1. **Finanzas**

- PredicciÃ³n de precios de acciones basado en indicadores
- EstimaciÃ³n de riesgo crediticio
- ValoraciÃ³n de activos

```python
Precio_AcciÃ³n = Î²â‚€ + Î²â‚ Ã— Volumen_Transacciones
```

### 2. **Inmobiliaria**

- TasaciÃ³n de propiedades
- PredicciÃ³n de alquileres

```python
Precio_Casa = Î²â‚€ + Î²â‚ Ã— TamaÃ±o_mÂ²
```

### 3. **Marketing**

- ROI de campaÃ±as publicitarias
- PredicciÃ³n de ventas

```python
Ventas = Î²â‚€ + Î²â‚ Ã— Gasto_Publicidad
```

### 4. **Salud**

- PredicciÃ³n de dosificaciÃ³n de medicamentos
- RelaciÃ³n entre edad y presiÃ³n arterial

```python
Dosis = Î²â‚€ + Î²â‚ Ã— Peso_Paciente
```

### 5. **EducaciÃ³n**

- PredicciÃ³n de calificaciones
- AnÃ¡lisis de factores de Ã©xito acadÃ©mico

```python
CalificaciÃ³n = Î²â‚€ + Î²â‚ Ã— Horas_Estudio
```

### 6. **Ciencia**

- AnÃ¡lisis de relaciones en experimentos
- CalibraciÃ³n de instrumentos

```python
Temperatura_Real = Î²â‚€ + Î²â‚ Ã— Lectura_Sensor
```

---

## ğŸ“Š Ejemplo Completo: Paso a Paso

### Contexto

Una empresa quiere predecir sus ventas mensuales basÃ¡ndose en el gasto en publicidad.

### Datos

```
Publicidad ($k): [10, 15, 20, 25, 30, 35, 40]
Ventas ($k):     [40, 55, 65, 75, 85, 95, 100]
```

### Paso 1: VisualizaciÃ³n

Crear un grÃ¡fico de dispersiÃ³n para verificar linealidad.

### Paso 2: CÃ¡lculo de la LÃ­nea

```
xÌ„ = 25
È³ = 73.57

m = Cov(x,y) / Var(x) â‰ˆ 2.29
b = È³ - mÃ—xÌ„ â‰ˆ 16.32

EcuaciÃ³n: Ventas = 16.32 + 2.29 Ã— Publicidad
```

### Paso 3: InterpretaciÃ³n

> "Por cada $1,000 adicionales en publicidad, las ventas aumentan en promedio $2,290."

### Paso 4: EvaluaciÃ³n

```
RÂ² â‰ˆ 0.95 â†’ Excelente ajuste
RMSE â‰ˆ 3.2k â†’ Error promedio de Â±$3,200
```

### Paso 5: PredicciÃ³n

```
Si Publicidad = $45k:
Ventas = 16.32 + 2.29 Ã— 45 = $119,370
```

### Paso 6: ValidaciÃ³n

Verificar residuos para asegurar que no hay patrones.

---

## ğŸ¯ Resumen de Conceptos Clave

### EcuaciÃ³n

```
y = mx + b
```

### Objetivo

Minimizar: Î£(yi - Å·i)Â²

### MÃ©tricas Principales

- **RÂ²**: Bondad de ajuste (0-1)
- **RMSE**: Error promedio en unidades de y
- **MAE**: Error absoluto promedio

### Supuestos

1. Linealidad
2. Independencia
3. Homoscedasticidad
4. Normalidad de residuos

### InterpretaciÃ³n

- **m**: Cambio en y por cada unidad de x
- **b**: Valor de y cuando x=0
- **RÂ²**: % de varianza explicada

### Residuos

- Diferencia entre real y predicho
- Deben distribuirse aleatoriamente
- Revelan problemas del modelo

---

## ğŸ’¡ Consejos Finales

1. **Siempre visualiza primero** - Los grÃ¡ficos revelan patrones
2. **Verifica supuestos** - No asumas que se cumplen
3. **Analiza residuos** - Te dicen si el modelo es apropiado
4. **No extrapoles lejos** - Las predicciones fuera del rango son riesgosas
5. **Contexto importa** - Un RÂ²=0.6 puede ser excelente o pÃ©simo segÃºn el dominio
6. **CorrelaciÃ³n â‰  CausaciÃ³n** - La regresiÃ³n no prueba causalidad

---

**Â¡Has completado la teorÃ­a de RegresiÃ³n Lineal Simple!** ğŸ‰

Ahora estÃ¡s listo para aplicar estos conceptos en la prÃ¡ctica. Recuerda: la regresiÃ³n lineal es simple pero poderosa cuando se usa correctamente.

**Siguiente paso**: Practica con el notebook guiado para ver estos conceptos en acciÃ³n.
